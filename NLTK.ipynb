{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WilliamShengYangHuang/RC18_Machine_Learning/blob/main/NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Toolkit (NLTK)"
      ],
      "metadata": {
        "id": "_uol05MFv6qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK (https://www.nltk.org/) is a suite of libraries and programmes for symbolic and ***statistical natural language processing*** for English written in the Python programming language. It supports classification, tokenisation, stemming, tagging, parsing, and semantic reasoning functionalities."
      ],
      "metadata": {
        "id": "bcO_WxjkwGOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "!pip install yake\n",
        "!pip install pyvirtualdisplay\n",
        "!sudo apt-get install xvfb\n",
        "!pip install spacy\n",
        "!pip install docx\n",
        "!pip install exceptions\n",
        "!pip install snownlp\n",
        "# !pip install googletrans\n",
        "!pip install translate\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install cairosvg"
      ],
      "metadata": {
        "id": "VyAsdzLB1l6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import yake\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.tree import Tree\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from graphviz import Source\n",
        "from graphviz import Digraph\n",
        "from IPython.display import Image, display\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import io\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "E30Petz81eRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tokenisation**"
      ],
      "metadata": {
        "id": "lilScJLqn0fE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenisation is a fundamental step in Natural Language Processing (NLP). It is ***the process of breaking down text into smaller pieces***, called tokens. These tokens help machines understand the context of the text. A token could be a word. The token could be a word, a group of words, sentence, paragraph, or an entire document to analyse.\n",
        "\n",
        "Tokenisation is necessary in NLP because it helps in interpreting the meaning of the text by analysing its semantics. It is easier to identify the base forms of words, the affixes attached to them, and the words’ context after tokenisation. Tokenisation is performed by ***locating word boundaries***. Ending point of a word and beginning of the next word is called word boundaries. Tokenisation is also about deciding where to separate the text based on specific characters such as whitespace, punctuation, etc."
      ],
      "metadata": {
        "id": "aViOAv50mPrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Isaura, city of the thousand wells, is said to rise over a deep, subterranean lake. On all sides, wherever the inhabitants dig long vertical holes in the ground, they succeed in drawing up water, as far as the city extends, and no farther. Its green border repeats the dark outline of the buried lake; an invisible landscape conditions the visible one; everything that moves in the sunlight is driven by the lapping wave enclosed beneath the rock's calcareous sky. Consequently two forms of religion exist in Isaura. The city's gods, according to some people, live in the depths, in the black lake that feeds the underground streams. According to others, the gods live in the buckets that rise, suspended from a cable, as they appear over the edge of the wells, in the revolving pulleys, in the windlasses of the norias, in the pump handles, in the blades of the windmills that draw the water up from the drillings, in the trestles that support the twisting probes, in the reservoirs perched on stilts over the roofs, in the slender arches of the aqueducts , in all the columns of water, the vertical pipes, the plungers, the drains, all the way up to the weathercocks that surmount the airy scaffoldings of Isaura, a city that moves entirely upward.\"\n"
      ],
      "metadata": {
        "id": "8f7cwWeN2HtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Extract the text from the document\n",
        "#text = \"According to others, the gods live in the buckets that rise, suspended from a cable, as they appear over the edge of the wells, in the revolving pulleys, in the windlasses of the norias, in the pump handles, in the blades of the windmills that draw the water up from the drillings, in the trestles that support the twisting probes, in the reservoirs perched on stilts over the roofs, in the slender arches of the aqueducts , in all the columns of water, the vertical pipes, the plungers, the drains, all the way up to the weathercocks that surmount the airy scaffoldings of Isaura, a city that moves entirely upward.\"\n",
        "\n",
        "# First extract the text from the document, then apply the language model to the text, and subsequently extract the words and their corresponding POS (Part-of-Speech) tags. Finally, display the dependency tree using the Spacy Python package.\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract the words and their corresponding POS tags ,Display the dependency tree\n",
        "pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "# Print the words and their POS tags\n",
        "for word, pos in pos_tags:\n",
        "    print(f\"'{word}': {pos}\")"
      ],
      "metadata": {
        "id": "iKU9BBqqX3Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependency Parsing"
      ],
      "metadata": {
        "id": "RlwiaFzu4VFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In NLP, there are several types of parse trees that you can generate, each providing different insights into the structure of a sentence. Here are a few examples:\n",
        "\n",
        "*   ***Dependency Parse Trees:*** These trees keep the words in the sentence as the nodes in the tree and use directed edges to show dependency relations between the words. Each edge is labeled with the type of grammatical relationship.\n",
        "*   ***Constituency Parse Trees:*** These trees break a text down into sub-phrases, or constituents. Non-terminal nodes in the tree are types of phrases, the terminal nodes are the words in the sentence, and the edges are unlabeled.\n",
        "*   ***Named Entity Recognition (NER) Trees:*** These are not traditional parse trees but are similar in that they show the structure of a sentence. In NER trees, the nodes are words or entities in the sentence, and the edges show the relationships between different entities.\n",
        "*   ***Semantic Role Labeling (SRL) Trees:*** These trees identify the semantic relationships among the words in a sentence. They label the sentence’s predicates (usually verbs) and their arguments (usually noun phrases).\n",
        "\n"
      ],
      "metadata": {
        "id": "pMY5Reg-31Pt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visulise Parsing Tree"
      ],
      "metadata": {
        "id": "mTKtxiPco7K0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Dependency Parsing*** is a technique in NLP that can reveal the grammatical relationships between words in a sentence. This analysis can help understand the structure and meaning of a sentence, thereby influencing the generation of text to image.\n",
        "\n",
        "Here are some ways Dependency Parsing might influence text to image generation:\n",
        "\n",
        "*   ***Improving Understanding of Object Relationships***: Dependency Parsing can help identify the main objects in a sentence and their relationships. For example, in the sentence “The cat is sitting on the chair”, Dependency Parsing can reveal that the “cat” is the main actor, the “chair” is the location, and “sitting on…” describes their relationship. This information can be used to generate an image more accurately.\n",
        "*   ***Identifying Important Details***: Dependency Parsing can also help identify adjectives that describe objects or actions, which can provide important details needed when generating an image. For example, in the sentence “The happy little dog is playing in the park”, “happy” and “little” describe the dog’s state and size, while “in the park” describes the scene. These details can help generate a more specific and vivid image.\n",
        "*   ***Resolving Ambiguity***: Sometimes, the structure of a sentence can lead to ambiguity, and Dependency Parsing can help resolve these ambiguities. For example, in the sentence “I saw her on the riverbank with a telescope”, is the “telescope” used for seeing, or is she on the riverbank with a telescope? Dependency Parsing can help determine the correct interpretation, thereby generating the correct image.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e4yGHFo2omHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save Dependency Parse Tree in SVG"
      ],
      "metadata": {
        "id": "mreQjR_6F7B5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the English language model\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Define the text you want to analyze\n",
        "text = \"Isaura, city of the thousand wells, is said to rise over a deep, subterranean lake. On all sides, wherever the inhabitants dig long vertical holes in the ground, they succeed in drawing up water, as far as the city extends, and no farther. Its green border repeats the dark outline of the buried lake; an invisible landscape conditions the visible one; everything that moves in the sunlight is driven by the lapping wave enclosed beneath the rock's calcareous sky. Consequently two forms of religion exist in Isaura. The city's gods, according to some people, live in the depths, in the black lake that feeds the underground streams. According to others, the gods live in the buckets that rise, suspended from a cable, as they appear over the edge of the wells, in the revolving pulleys, in the windlasses of the norias, in the pump handles, in the blades of the windmills that draw the water up from the drillings, in the trestles that support the twisting probes, in the reservoirs perched on stilts over the roofs, in the slender arches of the aqueducts , in all the columns of water, the vertical pipes, the plungers, the drains, all the way up to the weathercocks that surmount the airy scaffoldings of Isaura, a city that moves entirely upward.\"\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Apply the language model to the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Render the dependency tree as SVG\n",
        "svg = displacy.render(doc, style='dep', jupyter=False)\n",
        "\n",
        "# Parse the SVG string with BeautifulSoup\n",
        "soup = BeautifulSoup(svg, 'xml')\n",
        "\n",
        "# Select all text elements and change their font size\n",
        "for text in soup.find_all('text'):\n",
        "    text['style'] = 'font-size: 30px'  # Change to desired font size\n",
        "\n",
        "# Get the new SVG string\n",
        "new_svg = str(soup)\n",
        "\n",
        "# Save the SVG string as a file\n",
        "svg_filename = 'tree' #@param{type:'string'}\n",
        "svg_filepath = '/content/drive/MyDrive/' + svg_filename + '.svg'\n",
        "with open(svg_filepath, 'w') as f:\n",
        "    f.write(new_svg)\n",
        "\n",
        "# Display the SVG image\n",
        "from IPython.display import SVG, display\n",
        "display(SVG(filename=svg_filepath))"
      ],
      "metadata": {
        "id": "xmNF6jPAUqX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save Dependency Parse Tree in PNG"
      ],
      "metadata": {
        "id": "pRZP1ZNHFzZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load necessary libraries\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from bs4 import BeautifulSoup\n",
        "from PIL import Image\n",
        "import io\n",
        "import cairosvg\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Define the text you want to analyze\n",
        "text = \"This is a sample text for NLP analysis.\" #@param{type:'string'}\n",
        "\n",
        "# Apply the language model to the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Render the dependency tree as SVG\n",
        "svg = displacy.render(doc, style='dep', jupyter=False)\n",
        "\n",
        "# Parse the SVG string with BeautifulSoup\n",
        "soup = BeautifulSoup(svg, 'xml')\n",
        "\n",
        "# Select all text elements and change their font size\n",
        "for text_element in soup.find_all('text'):\n",
        "    text_element['style'] = 'font-size: 30px'  # Change to desired font size\n",
        "\n",
        "# Get the new SVG string\n",
        "new_svg = str(soup)\n",
        "\n",
        "# Convert SVG string to PNG using CairoSVG\n",
        "png = cairosvg.svg2png(bytestring=new_svg)\n",
        "\n",
        "# Convert PNG byte data to an image object\n",
        "image = Image.open(io.BytesIO(png))\n",
        "\n",
        "# Save the image object as a PNG file\n",
        "tree_image_file_name = 'dependency_tree' #@param{type:'string'}\n",
        "img_dir = '/content/drive/MyDrive/' + tree_image_file_name + '.png'\n",
        "image.save(img_dir)  # Replace with your preferred path\n",
        "\n",
        "# If you're using a Jupyter notebook or similar, you can display the image\n",
        "# image.show()\n"
      ],
      "metadata": {
        "id": "REPNmQvBErW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Apply the language model to the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Save the dependency tree to a file\n",
        "svg = displacy.render(doc, style='dep', jupyter=False)\n",
        "\n",
        "# Display the dependency tree\n",
        "# display(Image(data=svg))\n",
        "displacy.render(doc, style='dep', jupyter=True)\n",
        "\n",
        "# Save the SVG string as a file in google drive\n",
        "file_name = 'parse_tree_vec' #@param{type:'string'}\n",
        "saving_dir = '/content/drive/MyDrive/' + file_name + '.svg'\n",
        "with open(saving_dir, 'w') as f:\n",
        "    f.write(svg)"
      ],
      "metadata": {
        "id": "1IdbKK0lafIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Initialise preprocessing tool\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "80GpEnwP2EVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')  # Download the NLTK's \"punkt\" package (only need to do it once)\n",
        "\n",
        "sentences = nltk.tokenize.sent_tokenize(text)  # Split text into sentences\n",
        "# sentences = [nltk.tokenize.word_tokenize(s) for s in sentences]  # Split each sentence into words\n",
        "sentences"
      ],
      "metadata": {
        "id": "cUVwmNIoE1Vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenise text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Remove stop words and perform word stemming\n",
        "tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "\n",
        "# Using YAKE to determine keywords/descriptors\n",
        "kw_extractor = yake.KeywordExtractor()\n",
        "\n",
        "keywords = kw_extractor.extract_keywords(text)\n",
        "\n",
        "print(\"Key words:\", keywords) # A 'keywords' includes extracted keywords and their relevance score"
      ],
      "metadata": {
        "id": "PHHRszzS3g3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Cloud Visualisation"
      ],
      "metadata": {
        "id": "UsXYzZp46par"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the text you want to analyze\n",
        "text = \"Isaura, city of the thousand wells, is said to rise over a deep, subterranean lake. On all sides, wherever the inhabitants dig long vertical holes in the ground, they succeed in drawing up water, as far as the city extends, and no farther. Its green border repeats the dark outline of the buried lake; an invisible landscape conditions the visible one; everything that moves in the sunlight is driven by the lapping wave enclosed beneath the rock's calcareous sky. Consequently two forms of religion exist in Isaura. The city's gods, according to some people, live in the depths, in the black lake that feeds the underground streams. According to others, the gods live in the buckets that rise, suspended from a cable, as they appear over the edge of the wells, in the revolving pulleys, in the windlasses of the norias, in the pump handles, in the blades of the windmills that draw the water up from the drillings, in the trestles that support the twisting probes, in the reservoirs perched on stilts over the roofs, in the slender arches of the aqueducts , in all the columns of water, the vertical pipes, the plungers, the drains, all the way up to the weathercocks that surmount the airy scaffoldings of Isaura, a city that moves entirely upward.\" #@param{type:'string'}\n",
        "\n",
        "# Generate word cloud\n",
        "wordcloud = WordCloud(width = 800, height = 800,\n",
        "                background_color ='black',\n",
        "                stopwords = stop_words,\n",
        "                min_font_size = 50,\n",
        "                max_font_size = 500).generate(text)\n",
        "\n",
        "# Apply colourmap\n",
        "wordcloud = wordcloud.recolor(colormap='BrBG')\n",
        "\n",
        "# Visualise word cloud\n",
        "plt.figure(figsize = (8, 8), facecolor = None)\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HoImkJU-2Kon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Graph Visualisation"
      ],
      "metadata": {
        "id": "BlCMx8Xm6vcs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, nodes are individual words from the keywords, and edges are connections between keywords. This means that if two keywords share a word, there will be an edge between them. For example, if the keywords are “apple pie” and “pie chart”, then “apple”, “pie”, and “chart” will all be added as nodes, and there will be edges between “apple” and “pie”, and “pie” and “chart”."
      ],
      "metadata": {
        "id": "_MHVpGPhU4nW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenise text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Remove stop words and perform word stemming\n",
        "tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "\n",
        "# Using YAKE to determine keywords/descriptors\n",
        "kw_extractor = yake.KeywordExtractor()\n",
        "\n",
        "keywords = kw_extractor.extract_keywords(text)\n",
        "\n",
        "# Create graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes\n",
        "for keyword, score in keywords:\n",
        "    for word in nltk.word_tokenize(keyword):  # Split keywords into individual words\n",
        "        G.add_node(word, size=1/score)\n",
        "\n",
        "# Add edges\n",
        "for i in range(len(keywords)):\n",
        "    for j in range(i+1, len(keywords)):\n",
        "        for word1 in nltk.word_tokenize(keywords[i][0]):\n",
        "            for word2 in nltk.word_tokenize(keywords[j][0]):\n",
        "                G.add_edge(word1, word2)\n",
        "\n",
        "# Node size and colour\n",
        "sizes = [G.nodes[node]['size']*20 for node in G.nodes]\n",
        "colors = np.log([G.nodes[node]['size'] for node in G.nodes])\n",
        "\n",
        "# Calculate the importance of edges\n",
        "edge_importance = [(G.nodes[edge[0]]['size'] + G.nodes[edge[1]]['size']) / 2 for edge in G.edges]\n",
        "\n",
        "# Map importance scores to the colour space\n",
        "edge_colors = np.log(edge_importance)\n",
        "\n",
        "# Compute node position\n",
        "pos = nx.spring_layout(G, seed=42)\n",
        "\n",
        "# Set figure size\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "# draw graph\n",
        "nx.draw(G, pos, with_labels=True, node_size=sizes, node_color=colors, font_size=8, edge_color=edge_colors, edge_cmap=plt.cm.Greys, cmap=plt.cm.Blues)\n"
      ],
      "metadata": {
        "id": "uyqG4321a3Wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following case, nodes are the keywords themselves, and edges are connections between keywords. This means that each keyword is treated as a whole, and there is an edge between each pair of keywords. For example, if the keywords are “apple pie” and “pie chart”, then “apple pie” and “pie chart” will be added as nodes, and there will be an edge between them."
      ],
      "metadata": {
        "id": "TsIgqEf5U-cI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes\n",
        "for keyword, score in keywords:\n",
        "    G.add_node(keyword, size=1/score)\n",
        "\n",
        "# Add edges\n",
        "for i in range(len(keywords)):\n",
        "    for j in range(i+1, len(keywords)):\n",
        "        G.add_edge(keywords[i][0], keywords[j][0])\n",
        "\n",
        "# Node size and colour\n",
        "sizes = [G.nodes[node]['size']*20 for node in G.nodes]\n",
        "colors = np.log([G.nodes[node]['size'] for node in G.nodes])\n",
        "\n",
        "# Calculate the importance of edges\n",
        "edge_importance = [(G.nodes[edge[0]]['size'] + G.nodes[edge[1]]['size']) / 2 for edge in G.edges]\n",
        "\n",
        "# Map importance scores to the colour space\n",
        "edge_colors = np.log(edge_importance)\n",
        "\n",
        "# Compute node position\n",
        "pos = nx.spring_layout(G, seed=42)\n",
        "\n",
        "# Set figure size\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "# draw graph\n",
        "nx.draw(G, pos, with_labels=True, node_size=sizes, node_color=colors, font_size=8, edge_color=edge_colors, edge_cmap=plt.cm.Greys, cmap=plt.cm.Blues)"
      ],
      "metadata": {
        "id": "9owXzheW4olD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, The main difference between the two pieces of code is how nodes and edges are added to the graph. The choice between these two methods depends on how you want to represent your data. If you want to analyze the relationships between words, the first method might be more appropriate. If you want to analyze the relationships between keywords, the second method might be more appropriate."
      ],
      "metadata": {
        "id": "umITBSnUVIF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert NodeDataView to dictionary\n",
        "node_data = {node: attr for node, attr in G.nodes(data=True)}\n",
        "\n",
        "# Create a dataframe for the importance of nodes\n",
        "node_importance = pd.DataFrame.from_dict(node_data, orient='index').reset_index()\n",
        "node_importance.columns = ['Node', 'Attributes']\n",
        "node_importance['Importance'] = node_importance['Attributes'].apply(lambda x: x)\n",
        "\n",
        "# Create a dataframe for the importance of edges\n",
        "edge_importance_df = pd.DataFrame(edge_importance, columns=['Importance'])\n",
        "edge_importance_df['Edge'] = list(G.edges())\n",
        "\n",
        "# Reorder the columns\n",
        "edge_importance_df = edge_importance_df[['Edge', 'Importance']]\n",
        "\n",
        "# Print the dataframes\n",
        "print(node_importance)\n",
        "print(edge_importance_df)\n"
      ],
      "metadata": {
        "id": "ruUaG9uZefFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pearson Correlation Matrix: Edge Importance"
      ],
      "metadata": {
        "id": "KgLj62K5sjcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a new graph\n",
        "G_weighted = nx.Graph()\n",
        "\n",
        "# Add nodes\n",
        "for keyword, score in keywords:\n",
        "    G_weighted.add_node(keyword, size=1/score)\n",
        "\n",
        "# Add weighted edges\n",
        "for i in range(len(keywords)):\n",
        "    for j in range(i+1, len(keywords)):\n",
        "        # Use the importance of the edge as the weight\n",
        "        weight = (G_weighted.nodes[keywords[i][0]]['size'] + G_weighted.nodes[keywords[j][0]]['size']) / 2\n",
        "        G_weighted.add_edge(keywords[i][0], keywords[j][0], weight=weight)\n",
        "\n",
        "# Get the adjacency matrix with weights\n",
        "adj_matrix = nx.adjacency_matrix(G_weighted, weight='weight').todense()\n",
        "\n",
        "# Convert to a numpy array\n",
        "adj_array = np.array(adj_matrix)\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(adj_array, index=G_weighted.nodes(), columns=G_weighted.nodes())\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(df, annot=True, cmap='YlGnBu')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bEQhLPlpmKCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Hinton Diagram: Edge Importance"
      ],
      "metadata": {
        "id": "WoC-jKrLsc9R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def hinton(matrix, max_weight=None, ax=None):\n",
        "    \"\"\"Draw Hinton diagram for visualising a weight matrix.\"\"\"\n",
        "    ax = ax if ax is not None else plt.gca()\n",
        "\n",
        "    if not max_weight:\n",
        "        max_weight = 2**np.ceil(np.log(np.abs(matrix).max())/np.log(2))\n",
        "\n",
        "    ax.patch.set_facecolor('white')\n",
        "    ax.set_aspect('equal', 'box')\n",
        "\n",
        "    # Normalize the cell sizes\n",
        "    matrix = matrix / np.max(np.abs(matrix))\n",
        "\n",
        "    for (x, y), w in np.ndenumerate(matrix):\n",
        "        color = plt.cm.YlGnBu(np.abs(w))\n",
        "        size = np.sqrt(np.abs(w))\n",
        "        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n",
        "                             facecolor=color, edgecolor='white')  # Add white edges\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "    ax.autoscale_view()\n",
        "    ax.invert_yaxis()\n",
        "\n",
        "    # Add x-labels and y-labels\n",
        "    ax.set_xticks(np.arange(matrix.shape[1]))\n",
        "    ax.set_yticks(np.arange(matrix.shape[0]))\n",
        "    ax.set_xticklabels(df.columns, rotation=90)\n",
        "    ax.set_yticklabels(df.index)\n",
        "\n",
        "# Create a Hinton diagram\n",
        "plt.figure(figsize=(10, 7))\n",
        "hinton(df.values)\n",
        "plt.title('Hinton Diagram: Inter-Token Edge Importance')  # Add title\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "H4ZT5V39qXBN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}